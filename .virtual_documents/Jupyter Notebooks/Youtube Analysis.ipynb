


import pandas as pd
import matplotlib.pyplot as plt
import numpy as np


#load youtube data 

# Load the data into a DataFrame
yt_df = pd.read_csv("C:\\Users\\Ochieng' Oginga\\Documents\\Post_S\\Fall_2024\\Data_Science\\ds1-project\\Data_Files\\yt-crime_data_2.csv")

# Display the first few rows of the data
yt_df.head()



# Filter the data for videos published after 2023-09-01
filtered_yt_df = yt_df[yt_df['Published At'] > '2023-09-01']

# Check the filtered data
print(filtered_yt_df)


#using crime dictionary from scraping 
crime_keywords_dict = {
    "homicide": ["murder", "killing", "manslaughter", "shooting"],
    "sex abuse": ["rape", "assault", "domestic violence", "gender violence"],
    "assault with dangerous weapon": ["aggravated assault", "attempted manslaughter", "battery"],
    "robbery": ["theft", "mugging", "stealing", "robbing"],
    "burglary": ["break in", "forced entry", "housebreaking", "trespassing"],
    "theft auto": ["motor vehicle theft", "carjacking", "vehicle larceny"],
    "theft other": [],
    "arson": ["destruction of property", "set on fire"]
}



#distribution in perecentage 
# Initialize a dictionary to store counts for each crime category
crime_categories_counts = {category: 0 for category in crime_keywords_dict}

# Loop through the 'Description' column and count the occurrences of each category's keywords
for description in yt_df['Description']:
    if isinstance(description, str):  # Only process strings
        for category, keywords in crime_keywords_dict.items():
            for keyword in keywords:
                if keyword.lower() in description.lower():
                    crime_categories_counts[category] += 1
                    break  # No need to check further keywords for the current description
    else:
        continue  # Skip non-string descriptions

# Convert the dictionary to a DataFrame for easier visualization
category_df = pd.DataFrame(list(crime_categories_counts.items()), columns=['Crime Category', 'Count'])

# Calculate percentages based on total descriptions
total_descriptions = len(yt_df)
category_df['Percentage'] = (category_df['Count'] / total_descriptions) * 100

# Plot the results
plt.figure(figsize=(10, 6))
bars = plt.barh(category_df['Crime Category'], category_df['Percentage'], color='skyblue')

plt.xlabel('Percentage')
plt.title('Crime Category Occurrence in Video Descriptions (Percentage)')
plt.gca().invert_yaxis()  # To display the highest percentage at the top

# Add percentage values to the bars
for bar in bars:
    width = bar.get_width()  # Get the bar width (percentage value)
    plt.text(width + 0.5, bar.get_y() + bar.get_height()/2, f'{width:.2f}%', 
             va='center', ha='left', fontsize=10, color='black')

plt.show()



from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

# Preprocess the descriptions
yt_df['Description'] = yt_df['Description'].fillna('')  # Handle missing values
yt_df['Description'] = yt_df['Description'].str.lower()  # Convert to lowercase

# Vectorize the descriptions using TF-IDF
vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)  # Limit to top 1000 features
X = vectorizer.fit_transform(yt_df['Description'])

# Apply K-means clustering
n_clusters = 5  # You can adjust the number of clusters based on your needs
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
yt_df['Cluster'] = kmeans.fit_predict(X)

#Reduce dimensionality using PCA for visualization
pca = PCA(n_components=2)
reduced_data = pca.fit_transform(X.toarray())

# Visualize the clusters
plt.figure(figsize=(10, 6))
plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=yt_df['Cluster'], cmap='viridis', alpha=0.5)
plt.title('Clustering of Video Descriptions')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.colorbar(label='Cluster')
plt.show()

# Display top terms per cluster
terms = vectorizer.get_feature_names_out()
top_n = 10  # Display top 10 terms per cluster

for i in range(n_clusters):
    cluster_center = kmeans.cluster_centers_[i]
    top_terms_idx = cluster_center.argsort()[-top_n:][::-1]
    top_terms = [terms[idx] for idx in top_terms_idx]
    print(f"Cluster {i}: {', '.join(top_terms)}")



#Top Channels 
# Group by 'Channel Title' and count the number of videos for each channel
channel_counts = yt_df['Channel Title'].value_counts()

# Display the top 5 channels based on the video count
top_5_channels = channel_counts.head(5)

print(top_5_channels)



#top 5 channels by viewership 
# Group by 'Channel Title' and sum the 'Views' for each channel
channel_views = yt_df.groupby('Channel Title')['Views'].sum()

# Display the top 5 channels based on total viewership
top_5_channels_by_views = channel_views.sort_values(ascending=False).head(5)

print(top_5_channels_by_views)






#
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Data Cleaning and Preparation
yt_df = yt_df.dropna(subset=['Title', 'Description', 'Views', 'Likes'])  # Dropping rows with missing values

# Text Vectorization (TF-IDF for Title and Description)
vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
title_features = vectorizer.fit_transform(yt_df['Title'])
description_features = vectorizer.fit_transform(yt_df['Description'])

# Combine title and description features
from scipy.sparse import hstack
X = hstack([title_features, description_features])

# Target Variables
y_views = yt_df['Views']  # Target variable for views
y_likes = yt_df['Likes']  # Target variable for likes

# Train-Test Split
X_train, X_test, y_views_train, y_views_test = train_test_split(X, y_views, test_size=0.2, random_state=42)
_, _, y_likes_train, y_likes_test = train_test_split(X, y_likes, test_size=0.2, random_state=42)

# Build the Predictive Model (Random Forest Regressor)
rf_views = RandomForestRegressor(n_estimators=100, random_state=42)
rf_likes = RandomForestRegressor(n_estimators=100, random_state=42)

# Train models
rf_views.fit(X_train, y_views_train)
rf_likes.fit(X_train, y_likes_train)

# Make Predictions
y_views_pred = rf_views.predict(X_test)
y_likes_pred = rf_likes.predict(X_test)

#Evaluate the Model
print("Views Model Evaluation")
print("Mean Absolute Error (MAE):", mean_absolute_error(y_views_test, y_views_pred))
print("Mean Squared Error (MSE):", mean_squared_error(y_views_test, y_views_pred))
print("Root Mean Squared Error (RMSE):", np.sqrt(mean_squared_error(y_views_test, y_views_pred)))

print("\nLikes Model Evaluation")
print("Mean Absolute Error (MAE):", mean_absolute_error(y_likes_test, y_likes_pred))
print("Mean Squared Error (MSE):", mean_squared_error(y_likes_test, y_likes_pred))
print("Root Mean Squared Error (RMSE):", np.sqrt(mean_squared_error(y_likes_test, y_likes_pred)))

# Visualize Predictions vs Actuals
import matplotlib.pyplot as plt

# Views
plt.scatter(y_views_test, y_views_pred)
plt.xlabel("Actual Views")
plt.ylabel("Predicted Views")
plt.title("Actual vs Predicted Views")
plt.show()

# Likes
plt.scatter(y_likes_test, y_likes_pred)
plt.xlabel("Actual Likes")
plt.ylabel("Predicted Likes")
plt.title("Actual vs Predicted Likes")
plt.show()






from sklearn.metrics import mean_squared_error
def contains_crime_keywords(description, crime_keywords_dict):
    """Check if any keyword from crime_keywords_dict is present in the description"""
    for category, keywords in crime_keywords_dict.items():
        for keyword in keywords:
            if keyword in description.lower():
                return 1  # Mark as crime-related
    return 0  # Not crime-related

# Apply the function to label the data
yt_df['crime_related'] = yt_df['Description'].apply(lambda x: contains_crime_keywords(str(x), crime_keywords_dict))


# Initialize a TF-IDF Vectorizer
vectorizer_title = TfidfVectorizer(stop_words='english', max_features=5000)
vectorizer_description = TfidfVectorizer(stop_words='english', max_features=5000)

# Vectorize the Title and Description columns
title_features = vectorizer_title.fit_transform(yt_df['Title'])
description_features = vectorizer_description.fit_transform(yt_df['Description'])

# Combine title and description features into one matrix (same dimensions for train/test sets)
X = hstack([title_features, description_features])

# Target variables: Views and Likes
y_views = yt_df['Views']
y_likes = yt_df['Likes']


# Split the data into training and testing sets
X_train, X_test, y_train_views, y_test_views = train_test_split(X, y_views, test_size=0.2, random_state=42)
X_train, X_test, y_train_likes, y_test_likes = train_test_split(X, y_likes, test_size=0.2, random_state=42)

# Train a Random Forest Regressor for Views
rf_views = RandomForestRegressor(n_estimators=100, random_state=42)
rf_views.fit(X_train, y_train_views)

# Train a Random Forest Regressor for Likes
rf_likes = RandomForestRegressor(n_estimators=100, random_state=42)
rf_likes.fit(X_train, y_train_likes)


# Evaluate the model for Views (MSE and R-Squared)
y_pred_views = rf_views.predict(X_test)
mse_views = mean_squared_error(y_test_views, y_pred_views)
r2_views = rf_views.score(X_test, y_test_views)

# Evaluate the model for Likes (MSE and R-Squared)
y_pred_likes = rf_likes.predict(X_test)
mse_likes = mean_squared_error(y_test_likes, y_pred_likes)
r2_likes = rf_likes.score(X_test, y_test_likes)

# Print the results
print(f"Mean Squared Error (Views): {mse_views}")
print(f"R-Squared (Views): {r2_views}")
print(f"Mean Squared Error (Likes): {mse_likes}")
print(f"R-Squared (Likes): {r2_likes}")


#new video data
new_data = pd.DataFrame({
    'Title': ["Homicide reported in China Town DC"],
    'Description': ["A homicide was reported to MPD happening in China Town metro station early morning on Friday"]
})

# Vectorize the new data using the same vectorizer
new_title_features = vectorizer_title.transform(new_data['Title'])
new_description_features = vectorizer_description.transform(new_data['Description'])
new_X = hstack([new_title_features, new_description_features])

# Predict Views and Likes for the new video
predicted_views = rf_views.predict(new_X)
predicted_likes = rf_likes.predict(new_X)

print(f"Predicted Views: {predicted_views}")
print(f"Predicted Likes: {predicted_likes}")





#dictionary for locality 
crime_keywords_locality = {
    "northwest": ["northwest", "columbia heights", "nw"],
    "southwest": ["southwest", "sw"],
    "northeast": ["northeast", "ne"],
    "southeast": ["martin luther king avenue", "southeast", "se"]
}

# Another dictionary by key characteristics of the victim or perpetrator of the crime
crime_keywords_characteristics = {
    "adult_male": ["adult male", "adult man", "man"],
    "adult_female": ["adult female", "woman", "female", "women"],
    "teen_male": ["teen male", "male teen", "boy", "boys", "18-year-old man", "17-year-old male", "17-year-old man",
                  "19-year-old man", "19-year-old male", "16-year-old", "14-year-old", "teenager"],
    "teen_female": ["teen female", "female teen", "19-year-old woman", "18-year-old woman", "17-year-old woman",
                    "17-year-old girl", "17-year-old female", "16-year-old", "14-year-old", "15-year-old",
                    "teenager"],
    "child": ["child", "baby", "toddler", "kid", "two-year-old", "little girl", "little boy", "13-year-old", "children"]
}


# Initialize a dictionary to store counts for each locality-based crime category
locality_crime_counts = {category: 0 for category in crime_keywords_locality}

# Loop through the 'Description' column and categorize descriptions uniquely
for description in yt_df['Description']:
    if isinstance(description, str):  # Only process strings
        matched = False
        for category, keywords in crime_keywords_locality.items():
            for keyword in keywords:
                if keyword.lower() in description.lower():
                    locality_crime_counts[category] += 1
                    matched = True
                    break  # Stop checking keywords for the current category
            if matched:
                break  # Stop checking other categories if already matched
    else:
        continue  # Skip non-string descriptions

# Convert the dictionary to a DataFrame for easier visualization
locality_category_df = pd.DataFrame(list(locality_crime_counts.items()), columns=['Locality Crime Category', 'Count'])

# Calculate percentages based on total descriptions
total_descriptions = len(yt_df)
locality_category_df['Percentage'] = (locality_category_df['Count'] / total_descriptions) * 100

# Sort the DataFrame by the 'Percentage' column in ascending order
locality_category_df = locality_category_df.sort_values(by='Percentage', ascending=False)

# Plot the results
plt.figure(figsize=(10, 6))
bars = plt.barh(locality_category_df['Locality Crime Category'], locality_category_df['Percentage'], color='skyblue')
plt.xlabel('Percentage')
plt.title('Locality Crime Category Occurrence in Video Descriptions (Percentage)')
plt.gca().invert_yaxis()  # To display the highest percentage at the top

# Add percentage values to the bars
for bar in bars:
    width = bar.get_width()  # Get the bar width (percentage value)
    plt.text(width + 0.5, bar.get_y() + bar.get_height()/2, f'{width:.2f}%', 
             va='center', ha='left', fontsize=10, color='black')

plt.show()






# Initialize a dictionary to store counts for each characteristics-based crime category
characteristics_crime_counts = {category: 0 for category in crime_keywords_characteristics}

# Loop through the 'Description' column and count the occurrences of each category's keywords
for description in yt_df['Description']:
    if isinstance(description, str):  # Only process strings
        for category, keywords in crime_keywords_characteristics.items():
            for keyword in keywords:
                if keyword.lower() in description.lower():
                    characteristics_crime_counts[category] += 1
                    break  # Stop checking keywords for the current category
    else:
        # Skip non-string descriptions
        continue  

# Convert the dictionary to a DataFrame for easier visualization
characteristics_category_df = pd.DataFrame(list(characteristics_crime_counts.items()), columns=['Characteristics Crime Category', 'Count'])

# Calculate percentages based on total descriptions
total_descriptions = len(yt_df)
characteristics_category_df['Percentage'] = (characteristics_category_df['Count'] / total_descriptions) * 100

# Sort the DataFrame by the 'Percentage' column in ascending order
characteristics_category_df = characteristics_category_df.sort_values(by='Percentage', ascending=False)

# Plot the results
plt.figure(figsize=(10, 6))
bars = plt.barh(characteristics_category_df['Characteristics Crime Category'], 
                characteristics_category_df['Percentage'], 
                color='skyblue')

plt.xlabel('Percentage')
plt.title('Characteristics Crime Category Occurrence in Video Descriptions (Percentage)')
plt.gca().invert_yaxis()  # To display the highest percentage at the top

# Add percentage values to the bars
for bar in bars:
    width = bar.get_width()  # Get the bar width (percentage value)
    plt.text(width + 0.5, bar.get_y() + bar.get_height()/2, f'{width:.2f}%', 
             va='center', ha='left', fontsize=10, color='black')

plt.show()









print(yt_df.columns)


# List of keywords to look for in the 'Title' and 'Description'
keywords = ['homicide', 'sex abuse', 'assault with dangerous weapon', 'robbery', 
            'burglary', 'theft auto', 'theft other', 'arson']

# Create binary columns for each keyword in both 'Title' and 'Description'
for keyword in keywords:
    yt_df[keyword] = (
        yt_df['Title'].str.contains(keyword, case=False, na=False) | 
        yt_df['Description'].str.contains(keyword, case=False, na=False)
    ).astype(int)

# Set 'homicide' as the reference category
keywords_without_homicide = [kw for kw in keywords if kw != 'homicide']

# Define the independent variables (X) and dependent variable (y)
# Excludes 'homicide'
X = yt_df[keywords_without_homicide]  
# Add constant for the intercept
X = sm.add_constant(X)  
 # Dependent variable
y = yt_df['Views'] 

# Fit the OLS model
model = sm.OLS(y, X).fit()

# Show the summary
print(model.summary())


# Extract coefficients and confidence intervals from the model
coefficients = model.params
conf_int = model.conf_int()
conf_int.columns = ['Lower CI', 'Upper CI']

# Remove the 'const' term for plotting (optional)
coefficients = coefficients.drop('const')
conf_int = conf_int.drop('const')

# Plot coefficients with confidence intervals
plt.figure(figsize=(10, 6))
plt.errorbar(coefficients.index, coefficients, 
             yerr=(conf_int['Upper CI'] - conf_int['Lower CI']) / 2, 
             fmt='o', color='blue', ecolor='red', capsize=5)
# Reference line at 0
plt.axhline(0, color='gray', linestyle='--')  
plt.title('OLS Coefficients with 95% Confidence Intervals')
plt.xlabel('Keywords')
plt.ylabel('Coefficient Value')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()





# Define the independent variables (X) and dependent variable (y)
# Excludes 'homicide'
X = yt_df[keywords_without_homicide]  
# Add constant for the intercept
X = sm.add_constant(X)  
# Dependent variable
y = yt_df['Likes']  

# Fit the OLS model
model = sm.OLS(y, X).fit()

# Show the summary
print(model.summary())


# Extract coefficients and confidence intervals from the model
coefficients = model.params
conf_int = model.conf_int()
conf_int.columns = ['Lower CI', 'Upper CI']

# Remove the 'const' term for plotting
coefficients = coefficients.drop('const')
conf_int = conf_int.drop('const')

# Plot coefficients with confidence intervals
plt.figure(figsize=(10, 6))
plt.errorbar(coefficients.index, coefficients, 
             yerr=(conf_int['Upper CI'] - conf_int['Lower CI']) / 2, 
             fmt='o', color='blue', ecolor='red', capsize=5)

  # Reference line at 0
plt.axhline(0, color='gray', linestyle='--')
plt.title('OLS Coefficients for Likes with 95% Confidence Intervals')
plt.xlabel('Keywords')
plt.ylabel('Coefficient Value')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()




