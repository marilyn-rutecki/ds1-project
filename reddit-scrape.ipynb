{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65125cc8-56a5-468d-8720-4f8735f7cda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praw\n",
      "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting prawcore<3,>=2.4 (from praw)\n",
      "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting update_checker>=0.18 (from praw)\n",
      "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /opt/anaconda3/lib/python3.12/site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from prawcore<3,>=2.4->praw) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.8.30)\n",
      "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
      "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Installing collected packages: update_checker, prawcore, praw\n",
      "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#install praw\n",
    "\n",
    "#%pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "275de2c0-409d-4130-acf1-5bcd8a5a8d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import praw\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cab05972-aba6-4f30-999b-1042864d5d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#authorize praw\n",
    "reddit = praw.Reddit(\n",
    "    client_id= \"L9bDp7Cr1Uianzb7xtzeXw\",\n",
    "    client_secret= \"WCJSgyUcmO9hyu-ItBO1zAKiIX-sBQ\" , \n",
    "    user_agent= \"python:project-script:v1.0 (by u/Username10907)\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60f724c1-d2b9-4367-8794-53a61fcd91a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redditdev\n",
      "So many books, so little time\n",
      "###### [](#place announcements below)\n",
      "\n",
      "* New Release: [City of Night Birds by Juhea Kim](https://www.goodreads.com/search?&query=9780063394759)\n",
      "* Check out the [Weekly Recommendation Thread](https://redd.it/1gx59uk)\n",
      "* Join in the [Weekly \"What Are You Reading?\" Thread!](https://redd.it/1gzgayd)\n",
      "\n",
      "\n",
      "## [- Subreddit Rules -](/r/books/wiki/rules)[- Message the mods -](http://goo.gl/HXpfgH)[Related Subs](/r/books/wiki/relatedsubreddits)[AMA Info](/r/Books/wiki/amarules)[The FAQ](/r/books/wiki/faq) [The Wiki](/r/books/wiki/index)\n",
      "\n",
      "This is a moderated subreddit. It is our intent and purpose to foster and encourage in-depth discussion about all things related to books, authors, genres or publishing in a safe, supportive environment. If you're looking for help with a personal book recommendation, consult our [Suggested Reading](/r/books/wiki/suggested) page or ask in: /r/suggestmeabook\n",
      "\n",
      "# Quick Rules:\n",
      "\n",
      "1. **Discussion is the goal**  \n",
      "Do not post shallow content. All posts must be directly book related, informative, and discussion focused. \n",
      "\n",
      "2. **Personal conduct**  \n",
      "Please use a civil tone and assume good faith when entering a conversation.\n",
      "\n",
      "3. **Prohibited**  \n",
      "Promotional posts, comments & flairs, media-only posts, personalized recommendation requests incl. ‘Should I read …?’, ‘What’s that book?’ posts, sales links, piracy, plagiarism, low quality book lists, unmarked spoilers (instructions for spoiler tags are in the sidebar), sensationalist headlines, novelty accounts, low effort content. *Please see extended rules for appropriate alternative subreddits, like /r/suggestmeabook, /r/whatsthatbook, etc. or check out our [Related Subreddits](/r/books/wiki/relatedsubreddits).*\n",
      "\n",
      "4. **Encouraged**  \n",
      "We love *original* content and self-posts! Thoughts, discussion questions, epiphanies and interesting links about authors and their work. We also encourage discussion about developments in the book world and we have a flair system. \n",
      "\n",
      "5. **Important**  \n",
      "We don't allow personal recommendation posts. You can ask in our Weekly Recommendation Thread, consult our [Suggested Reading](/r/books/wiki/suggested) or [What to Read](/r/books/wiki/whattoread) page, or post in /r/suggestmeabook.\n",
      "\n",
      "6. **[Click here for the extended rules](/r/Books/wiki/rules)**  \n",
      "Please report any comment that does not follow the rules and remember that mods have the final say.\n",
      "\n",
      "---\n",
      "\n",
      "# Weekly Thread Calendar\n",
      "Day|Frequency|Feature\n",
      ":--:|:--:|:--:\n",
      "Monday|Weekly|[What Books did You Start or Finish Reading this Week?: November 25, 2024](https://redd.it/1gzgayd)\n",
      "Tuesday|1st of the month|[New Releases: October 2024](https://redd.it/1ftjlb8)\n",
      "Wednesday|Weekly|Literature of the World: [Literature of Slovakia: August 2024](https://redd.it/1f37a75)\n",
      "Thursday|Weekly|Genre Discussion: [Favorite Books with Bullies: November 2024](https://redd.it/1gr37x8)\n",
      "Friday|Weekly|[Weekly Recommendation Thread: November 22, 2024](https://redd.it/1gx59uk)\n",
      "Sunday|Weekly|[Weekly FAQ Thread November 24, 2024: How do I better understand the book I'm reading?](https://redd.it/1gypf00)\n",
      "Tues/Sat|Bi-Weekly|[Simple Questions: November 26, 2024](https://redd.it/1h08e0h)\n",
      "\n",
      "---\n",
      "\n",
      "# Upcoming AMAs\n",
      " | | |\n",
      ":-:|:-:|:-:\n",
      "\n",
      "[The Complete AMA Schedule](/r/books/wiki/amafullschedule)\n",
      "\n",
      "# [Related Subreddits](https://goo.gl/0jV4RT):\n",
      "### [Discussion](https://goo.gl/gc2SkD)\n",
      "### [Genres](https://goo.gl/tqAz83)\n",
      "### [Images](https://goo.gl/coikuy)\n",
      "### [Writing](https://goo.gl/JqC3qe)\n",
      "### [eBooks](https://goo.gl/gxn0Qi)\n",
      "### [Authors](https://goo.gl/VJn5UZ)\n",
      "### [Books/Series](https://goo.gl/DGbvQI)\n",
      "\n",
      "# Other Links:\n",
      "#### Follow our [official Twitter](http://goo.gl/RBcyna) for updates on AMAs and the day's most popular posts!\n",
      "\n",
      "# Spoiler Policy:\n",
      "- Any post with a spoiler in the title will be removed.\n",
      "- Any comment with a spoiler that doesn't use the spoiler code will be removed.\n",
      "- Any user with an extensive history of spoiling books will be banned.\n",
      "- Spoiler tags cover spoilers with black bars that reveal spoilers when a cursor hovers over them They are written as: \\>!spoiler!< with the text \"spoiler\" being your spoiler. Example: >!Hello.!<\n",
      "\n",
      "# [Explanation of our link flairs](/r/books/wiki/flairs)\n",
      "\n",
      "# [Join our /r/bookclub](/r/bookclub)\n",
      "\n",
      "# [Don't forget /new!](/r/books/new)\n",
      "\n",
      "# Filter by Flair\n",
      "> # [AMA](http://goo.gl/94lql6)\n",
      "> # [Weekly Thread](https://www.reddit.com/r/books/search?q=flair%3Aweeklythread&restrict_sr=on&sort=new&t=all)\n",
      "> # [Mod Post](/r/books/search?sort=new&restrict_sr=on&q=flair%3Amod%20post)\n",
      "\n",
      "---\n",
      "\n",
      "> ###### [](#the book banner)\n",
      ">\n",
      "> * [bsct](https://reddit.com/comments/1gu27lh/x/lxr2cz2)\n",
      "* [](https://reddit.com/comments/1gu27lh/x/lyrm961)\n",
      "* [](https://reddit.com/comments/1gu27lh/x/lxqpbo5)\n",
      "* [](https://reddit.com/comments/1gu27lh/x/ly36340)\n",
      "* [](https://reddit.com/comments/1gu27lh/x/ly1p8uw)\n",
      "* [](https://reddit.com/comments/1gu27lh/x/lxs9spz)\n",
      "* [](https://reddit.com/comments/1gu27lh/x/lxx5mxo)\n",
      "* [](https://reddit.com/comments/1gu27lh/x/lxs3z86)\n",
      "* [](https://reddit.com/comments/1gu27lh/x/ly2j5rb)\n",
      "* [](https://reddit.com/comments/1gu27lh/x/lxrqe0i)\n",
      "* [](https://reddit.com/comments/1gu27lh/x/lxtln2p)\n",
      "* [](https://reddit.com/comments/1gu27lh/x/lxrynwr)\n",
      "* [](https://reddit.com/comments/1gu27lh/x/lxqv9mj)\n",
      "* [](https://reddit.com/comments/1gu27lh/x/ly2j5rb)\n",
      "* [](https://reddit.com/comments/1gu27lh/x/lxrqe0i)\n",
      "* [](https://reddit.com/comments/1gu27lh/x/lxrgh2b)\n",
      "* [](https://reddit.com/comments/1gu27lh/x/lxs0ffk)\n",
      "* [](https://reddit.com/comments/1gu27lh/x/lxrqe0i)\n",
      "* [](https://reddit.com/comments/1gu27lh/x/lxqo0cf)\n",
      "* [](https://reddit.com/comments/1gu27lh/x/lxr2cz2)\n",
      "* [](https://reddit.com/comments/1gu27lh/x/lyr8k7c)\n",
      "* [](https://reddit.com/comments/1gu27lh/x/lxtpdg0)\n",
      "* [](https://reddit.com/comments/1gu27lh/x/lyjtp0b)\n",
      "* [](https://reddit.com/comments/1gu27lh/x/lyjtp0b)\n",
      "* [](https://reddit.com/comments/1gu27lh/x/lxs9spz)\n",
      "* [](https://reddit.com/comments/1gu27lh/x/lxqyki9)\n",
      "* [](https://reddit.com/comments/1gu27lh/x/lxsdubp)\n",
      "* [](https://reddit.com/comments/1gu27lh/x/lxqr470)\n",
      "* [](https://reddit.com/comments/1gu27lh/x/lxsp458)\n",
      "* [](https://reddit.com/comments/1gu27lh/x/lyukd75)\n",
      "\n",
      "\n",
      "\n",
      "##### [ama](https://www.goodreads.com/search?&query=9780063394759)\n",
      "\n",
      "\n",
      "# Check out this week's [Thread Calendar](https://redd.it/1gzgaw8)\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "test_subreddit = reddit.subreddit(\"redditdev\")\n",
    "print(test_subreddit.display_name)\n",
    "\n",
    "book_subreddit = reddit.subreddit(\"books\")\n",
    "print(book_subreddit.title)\n",
    "print(book_subreddit.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "465b6a42-9803-4f27-b47c-889bfaae2575",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pulling from r/washingtondc\n",
    "\n",
    "my_subreddit = \"washingtondc\"\n",
    "submission_headers = ['author', 'created_utc', 'id', \n",
    "                      'is_original_content', 'is_self', \n",
    "                      'link_flair_text', 'locked', 'name', \n",
    "                      'num_comments', 'over_18', 'permalink', \n",
    "                      'score', 'selftext', 'spoiler', 'stickied', \n",
    "                      'subreddit', 'title', 'upvote_ratio', 'url']\n",
    "\n",
    "#Note: 'a' opens the file in append mode to avoid overwriting data\n",
    "with open(\"reddit_test_submission_db.csv\", 'a', \n",
    "          encoding=\"utf-8\", newline='') as f_object:\n",
    "    newposts = reddit.subreddit(my_subreddit).new(limit=None)\n",
    "    for post in newposts:\n",
    "    #Below are all the fields we'll request from PRAW for each post\n",
    "        data = {'author': post.author, 'created_utc': \n",
    "                post.created_utc, 'id': post.id, \n",
    "                'is_original_content': post.is_original_content, \n",
    "                'is_self': post.is_self, 'link_flair_text': \n",
    "                post.link_flair_text, 'locked': post.locked, \n",
    "                'name': post.name, 'num_comments': \n",
    "                post.num_comments, 'over_18': post.over_18, \n",
    "                'permalink': post.permalink, 'score': post.score, \n",
    "                'selftext': post.selftext, 'spoiler': post.spoiler, \n",
    "                'stickied': post.stickied, 'subreddit': \n",
    "                post.subreddit, 'title': post.title, \n",
    "                'upvote_ratio': post.upvote_ratio, 'url': post.url}\n",
    "        dictwriter_object = csv.DictWriter(\n",
    "            f_object, fieldnames=submission_headers)\n",
    "        dictwriter_object.writerow(data)\n",
    "    f_object.close()\n",
    "    \n",
    "#Code below will delete duplicates on successive pulls\n",
    "post_db = pd.read_csv(\"reddit_test_submission_db.csv\", \n",
    "                      names=submission_headers, header=0)\n",
    "post_db.drop_duplicates(subset=\"permalink\", \n",
    "                        keep=\"last\", inplace=True)\n",
    "post_db.to_csv(\"reddit_test_submission_db.csv\", \n",
    "               index=False, chunksize=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff9d773-21a3-4bb3-94ed-72cbbd84f45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This block creates list of submissions for which we want comments\n",
    "comment_headers = ['author', 'body', 'created_utc', \n",
    "                   'distinguished', 'edited', 'id', \n",
    "                   'is_submitter', 'link_id', 'parent_id', \n",
    "                   'permalink', 'saved', 'score', 'stickied', \n",
    "                   'submission', 'subreddit', 'subreddit_id']\n",
    "with open('reddit_test_comment_db.csv', 'a') as comment_file:\n",
    "    comments_db = pd.read_csv('reddit_test_comment_db.csv', \n",
    "                              usecols=[\"submission\"], \n",
    "                              names=comment_headers)\n",
    "    comment_file.close()\n",
    "submission_db = pd.read_csv(\"reddit_test_submission_db.csv\", \n",
    "                            usecols=[\"created_utc\", \"id\"])\n",
    "#Filter down to submissions for which we don't yet have comments\n",
    "comments_set = set(comments_db[\"submission\"])\n",
    "#259,200 is three days worth of seconds\n",
    "try:\n",
    "    time_cutoff = max([created_utc for post_id, created_utc in \n",
    "         zip(submission_db.id, submission_db.created_utc) \n",
    "         if post_id in comments_set]) - 259200\n",
    "except:\n",
    "    time_cutoff = submission_db[\"created_utc\"].min()\n",
    "submissions_to_pull = submission_db.loc[submission_db\n",
    "                                        [\"created_utc\"] >= \n",
    "                                        time_cutoff, \"id\"]\n",
    "#This block pulls all comments for the list of submissions we have identified\n",
    "with open(\"reddit_test_comment_DB.csv\", 'a', \n",
    "          encoding=\"utf-8\", newline='') as f_object:\n",
    "    for row in submissions_to_pull:\n",
    "        submission = reddit.submission(id=row)\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        for comment in submission.comments.list():\n",
    "            data = {'author': comment.author, 'body': \n",
    "                    comment.body, 'created_utc': \n",
    "                    comment.created_utc, 'distinguished': \n",
    "                    comment.distinguished, 'edited': \n",
    "                    comment.edited, 'id': comment.id, \n",
    "                    'is_submitter': comment.is_submitter, \n",
    "                    'link_id': comment.link_id, 'parent_id': \n",
    "                    comment.parent_id, 'permalink': \n",
    "                    comment.permalink, 'saved': comment.saved, \n",
    "                    'score': comment.score, 'stickied': \n",
    "                    comment.stickied, 'submission': \n",
    "                    comment.submission, 'subreddit': \n",
    "                    comment.subreddit, \n",
    "                    'subreddit_id': comment.subreddit_id}\n",
    "            dictwriter_object = csv.DictWriter(f_object, \n",
    "                                               fieldnames=\n",
    "                                               comment_headers)\n",
    "            dictwriter_object.writerow(data)\n",
    "    f_object.close()\n",
    "    \n",
    "#Now drop duplicate rows\n",
    "comment_db = pd.read_csv(\"reddit_test_comment_DB.csv\", \n",
    "                         names=comment_headers, header=0)\n",
    "#First drop duplicates that are not edited, keep the last pull\n",
    "comment_db.drop_duplicates(subset=[\"permalink\", \"body\", \"id\"],\n",
    "                           keep = \"last\", inplace=True)\n",
    "#Then drop duplicates that have been edited, keep the first pull\n",
    "comment_db.drop_duplicates(subset=\"permalink\", \n",
    "                           keep = \"first\", inplace=True)\n",
    "comment_db.to_csv(\"reddit_test_comment_DB.csv\", \n",
    "                  index=False, chunksize=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
