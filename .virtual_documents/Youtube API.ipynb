


#import needed packages 
from googleapiclient.discovery import build
import pandas as pd

# load keys from  environmental var
from dotenv import load_dotenv
import os


# Load .env file
load_dotenv()

# Retrieve the API key
#api_key = os.getenv("API_KEY")
#second API key 
#api_key = os.getenv("API_KEY2")
#third API key 
api_key = os.getenv("API_KEY3")
#print(api_key)  


#instantiate a client 
youtube = build('youtube', 'v3', developerKey=api_key)


#create crime keywords in lower case
crime_keywords = [
    "homicide", "murder", "killing", "manslaughter", "shooting",
    "sex abuse", "rape", "assault", "domestic violence", "gender violence",
    "assault with dangerous weapon", "aggravated assault", "attempted manslaughter", "battery",
    "robbery", "theft", "mugging", "stealing", "robbing",
    "burglary", "break in", "forced entry", "housebreaking", "tresspassing",
    "theft auto", "motor vehicle theft", "carjacking", "vehicle larceny",
    "theft other",
    "motor vehicle theft",
    "arson", "destruction of property", "set on fire"
]

#dc names 
dc_names = [
    "Washington DC", "Washington D.C.", "DC", "D.C.", "District of Columbia",
    "The District", "District", "Capital City", "The Capital",
]




#import time

# Function to search for videos (with pagination support and delay)
def search_youtube(query):
    all_videos = []
    next_page_token = None

    while len(all_videos) < 500:  # Adjust the limit to 5,000 videos
        request = youtube.search().list(
            part="snippet",
            q=query,
            type="video",
            maxResults=50,  # Max allowed per request
            pageToken=next_page_token  # Token for the next page
        )
        response = request.execute()

        # Add videos from the current page to the list
        all_videos.extend(response['items'])

        # Check if there's another page
        next_page_token = response.get('nextPageToken')
        if not next_page_token:
            break  # Exit if there are no more pages

       # time.sleep(1)  # Add a 1-second delay between requests to avoid rate limits

    return all_videos[:500]  # Limit to 5,000 videos

# Function to get video details (like count, view count)
#def get_video_details(video_id):
   # request = youtube.videos().list(
   #     part="statistics",
   #     id=video_id
   # )
   # response = request.execute()
    #return response['items'][0]['statistics']

# Loop over each combination of dc_names and crime_keywords
def scrape_videos():
    all_videos = []
    
    for dc_name in dc_names:
        for crime_keyword in crime_keywords:
            query = f"{dc_name} {crime_keyword}"
            print(f"Searching for: {query}")
            videos = search_youtube(query)
            
            for video in videos:
                video_id = video['id']['videoId']
                stats = get_video_details(video_id)
                
                # Only keep videos with more than 100 views
                view_count = int(stats.get('viewCount', 0))
                if view_count > 100:
                    video['views'] = view_count  # Add view count to the video data
                    all_videos.append(video)
            
            # Stop if we've reached 500 videos
            if len(all_videos) >= 500:
                break
        if len(all_videos) >= 500:
            break
    
    return all_videos[:500]  # Limit to 5,000 videos



#skip the next chunk 


# Function to get video details (like count, view count, description, comments, and publication date)
def get_video_details(video_id):
    request = youtube.videos().list(
        part="snippet,statistics",  # 'snippet' for description, 'statistics' for views, comments, etc.
        id=video_id
    )
    response = request.execute()
    video_data = response['items'][0]
    
    # Extract relevant details
    stats = video_data['statistics']
    snippet = video_data['snippet']
    
    video_details = {
        'viewCount': int(stats.get('viewCount', 0)),
        'commentCount': int(stats.get('commentCount', 0)),
        'description': snippet.get('description', ''),
        'publishedAt': snippet.get('publishedAt', ''),
        'likeCount': int(stats.get('likeCount', 0)),
    }
    
    return video_details

# Scrape and store results in DataFrame
def scrape_videos_to_df():
    all_videos_data = []
    
    videos = scrape_videos()  # Call the scrape_videos function
    
    for video in videos:
        title = video['snippet']['title']
        video_id = video['id']['videoId']
        
        # Get additional video details
        video_details = get_video_details(video_id)
        
        # Collecting video data into a dictionary
        video_data = {
            'Title': title,
            'URL': f"https://www.youtube.com/watch?v={video_id}",
            'Likes': video_details['likeCount'],
            'Views': video_details['viewCount'],
            'Comments': video_details['commentCount'],
            'Description': video_details['description'],
            'Published At': video_details['publishedAt']
        }
        
        # Add the video data to the list
        all_videos_data.append(video_data)
    
    # Convert the list of dictionaries to a DataFrame
    yt_crime_data = pd.DataFrame(all_videos_data)
    
    return yt_crime_data

# Get the data and display it
yt_crime_data = scrape_videos_to_df()

# Print the DataFrame (optional)
print(yt_crime_data)





# Function to get video details (like count, view count, description, comments, and publication date, plus channel info)
def get_video_details(video_id):
    request = youtube.videos().list(
        part="snippet,statistics",  # 'snippet' for description, 'statistics' for views, comments, etc.
        id=video_id
    )
    response = request.execute()
    video_data = response['items'][0]
    
    # Extract relevant details
    stats = video_data['statistics']
    snippet = video_data['snippet']
    
    # Include channel details
    video_details = {
        'viewCount': int(stats.get('viewCount', 0)),
        'commentCount': int(stats.get('commentCount', 0)),
        'description': snippet.get('description', ''),
        'publishedAt': snippet.get('publishedAt', ''),
        'likeCount': int(stats.get('likeCount', 0)),
        'channelId': snippet.get('channelId', ''),  # Channel ID
        'channelTitle': snippet.get('channelTitle', '')  # Channel Title
    }
    
    return video_details



# Scrape and store results in DataFrame
def scrape_videos_to_df():
    all_videos_data = []
    
    videos = scrape_videos()  # Call the scrape_videos function
    
    for video in videos:
        title = video['snippet']['title']
        video_id = video['id']['videoId']
        
        # Get additional video details (including channel info)
        video_details = get_video_details(video_id)
        
        # Collecting video data into a dictionary
        video_data = {
            'Title': title,
            'URL': f"https://www.youtube.com/watch?v={video_id}",
            'Likes': video_details['likeCount'],
            'Views': video_details['viewCount'],
            'Comments Count': video_details['commentCount'],
            'Description': video_details['description'],
            'Published At': video_details['publishedAt'],
            'Channel ID': video_details['channelId'],  # Channel ID
            'Channel Title': video_details['channelTitle']  # Channel Title
        }
        
        # Add the video data to the list
        all_videos_data.append(video_data)
    
    # Convert the list of dictionaries to a DataFrame
    yt_crime_data = pd.DataFrame(all_videos_data)
    
    return yt_crime_data
# Get the data and display it
yt_crime_data = scrape_videos_to_df()

# Print the DataFrame (optional)
print(yt_crime_data)


# Save the DataFrame to a CSV file
yt_crime_data.to_csv('yt_crime_data_2.csv', index=False)





#scraping the next 500 videos 


# yt_crime_data existing DataFrame with previously scraped videos
def scrape_videos_to_df():
    all_videos_data = []
    
    # Assuming you already have a DataFrame `yt_crime_data` from previous scrapes
    existing_video_ids = yt_crime_data['URL'].apply(lambda x: x.split('v=')[-1]).tolist()  # Extract existing video IDs
    
    # Call the scrape_videos function to get new videos
    videos = scrape_videos()
    
    for video in videos:
        video_id = video['id']['videoId']
        
        # Skip videos that are already in the DataFrame (by checking the video ID)
        if video_id in existing_video_ids:
            continue
        
        # Get additional video details (including channel info)
        video_details = get_video_details(video_id)
        
        # Collecting video data into a dictionary
        video_data = {
            'Title': video['snippet']['title'],
            'URL': f"https://www.youtube.com/watch?v={video_id}",
            'Likes': video_details['likeCount'],
            'Views': video_details['viewCount'],
            'Comments Count': video_details['commentCount'],
            'Description': video_details['description'],
            'Published At': video_details['publishedAt'],
            'Channel ID': video_details['channelId'],  # Channel ID
            'Channel Title': video_details['channelTitle']  # Channel Title
        }
        
        # Add the video data to the list
        all_videos_data.append(video_data)
    
    # Convert the list of dictionaries to a DataFrame
    new_videos_data = pd.DataFrame(all_videos_data)
    
    # Append new videos to the existing DataFrame
    yt_crime_data = pd.concat([yt_crime_data, new_videos_data], ignore_index=True)
    
    return yt_crime_data

# Assuming yt_crime_data is the DataFrame from previous scrapes
yt_crime_data = scrape_videos_to_df()

# Print the updated DataFrame (optional)
print(yt_crime_data)



